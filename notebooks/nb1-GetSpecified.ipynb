{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle New Data Requests Automatically\n",
    "- beginning of notebook is assumed to be interactive until the requests have been checked\n",
    "- all progress and exception logging is done only for main loop\n",
    "- copy and paste the e-mail response and send from gcs.cmip6.ldeo@gmail.com account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "from IPython.display import display\n",
    "from glob import glob\n",
    "import warnings\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from request import requests, set_request_id\n",
    "from search import esgf_search, esgf_search_sites\n",
    "from netcdf import get_ncfiles, concatenate\n",
    "from identify import needed\n",
    "from response import response, dict_to_dfcat, get_details\n",
    "from utilities import getFolderSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to write local zarr stores:\n",
    "zarr_local = '/h85/naomi/zarr-minimal'\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose basic configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = esgf_search_sites()\n",
    "print('possible ESGF API search nodes: ',list(dtype.keys()))\n",
    "\n",
    "local_node = False\n",
    "ESGF_site = dtype['llnl'];local_node = True\n",
    "#ESGF_site = dtype['dkrz']\n",
    "#ESGF_site = dtype['ipsl']\n",
    "#ESGF_site = dtype['ceda'];local_node = False  # CEDA doesn't allow local-only searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete archive of CMIP6 output is made available for search and download via any one of the following portals:\n",
    "\n",
    "USA, PCMDI/LLNL (California) - https://esgf-node.llnl.gov/search/cmip6/\n",
    "\n",
    "France, IPSL - https://esgf-node.ipsl.upmc.fr/search/cmip6-ipsl/\n",
    "\n",
    "Germany, DKRZ - https://esgf-data.dkrz.de/search/cmip6-dkrz/\n",
    "\n",
    "UK, CEDA - https://esgf-index1.ceda.ac.uk/search/cmip6-ceda/\n",
    "\n",
    "If you encounter slow responses from one search interface, you might try one of the other portals (perhaps one near you). Also note that the datasets themselves are stored (and partially replicated) on a federated system of data nodes, and again you may find differences from node to node in download speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ESGF = True\n",
    "\n",
    "# must choose ONE table_id\n",
    "#table_id = 'Amon'\n",
    "table_id = 'day'\n",
    "\n",
    "#must choose LIST of experiments, variables\n",
    "#experiment_ids = ['ssp245', 'ssp370', 'ssp585']\n",
    "experiment_ids = ['piControl']\n",
    "variable_ids = ['prc']\n",
    "#variable_ids = ['hus','mrsos','prc','rlds','sfcWind','ua','va','wap']\n",
    "\n",
    "# can specify 'All' or give a list\n",
    "sources = ['CESM2-WACCM-FV2']\n",
    "#sources = 'All'\n",
    "members = ['r1i1p1f1']\n",
    "#members = 'All'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search ESGF for the availability of requested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_ESGF:\n",
    "    # get for all sources and members, filter request later\n",
    "    df_list = []\n",
    "    for experiment_id in experiment_ids:\n",
    "        for variable_id in variable_ids:\n",
    "            print(experiment_id,variable_id)\n",
    "            try:\n",
    "                files= esgf_search(server=ESGF_site, mip_era='CMIP6', variable_id=variable_id, \n",
    "        table_id=table_id, experiment_id=experiment_id, page_size=500, verbose=False, local_node=False)\n",
    "                #print('got-em')\n",
    "            except:\n",
    "                #print(experiment_id, table_id,variable_id,'nothing doing')\n",
    "                continue\n",
    "\n",
    "            files.loc[:,'version'] = [str.split('/')[-2] for str in files['HTTPServer_url']]\n",
    "            files.loc[:,'file_name'] = [str.split('/')[-1] for str in files['HTTPServer_url']]\n",
    "            # might need to set activity_id to activity_drs for some files (see old versions)\n",
    "            files.loc[:,'activity_id'] = files.activity_drs\n",
    "            df_list += [files.drop_duplicates(subset =[\"file_name\",\"version\",\"checksum\"]) ]\n",
    "\n",
    "    dESGF = pd.concat(df_list,sort=False)\n",
    "    dESGF = dESGF.drop_duplicates(subset =[\"file_name\",\"version\",\"checksum\"])\n",
    "    dESGF.to_csv('csv/ESGF_specific.csv',index=False)\n",
    "else:\n",
    "    dESGF = pd.read_csv('csv/ESGF_specific.csv')\n",
    "    dESGF = dESGF.drop_duplicates(subset =[\"file_name\",\"version\",\"checksum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the master list of existing zarr stores\n",
    "- df_avail includes all stores, EVEN THOSE with known ES-DOC issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dzLocal = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype={'version': 'unicode'})\n",
    "len(dzLocal),len(dESGF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the new requests:\n",
    "- already exists in df_avail (what we have) - not needed\n",
    "- exists in df_ESGF (what is available) - if not available, then not needed|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_files_needed = True\n",
    "if update_files_needed:\n",
    "    ngood = 0\n",
    "    zarr_format = '/%(activity_drs)s/%(institution_id)s/%(source_id)s/%(experiment_id)s/\\\n",
    "%(member_id)s/%(table_id)s/%(variable_id)s/%(grid_label)s/'\n",
    "    df_list = []\n",
    "    if sources == 'All':\n",
    "        source_ids = dESGF.source_id.unique()\n",
    "    else:\n",
    "        source_ids = sources\n",
    "    for source_id in source_ids:\n",
    "        for experiment_id in experiment_ids:\n",
    "            print(experiment_id,source_id)\n",
    "            for variable_id in variable_ids:\n",
    "                df_grid = dESGF[(dESGF.experiment_id==experiment_id)&(dESGF.source_id==source_id)&\n",
    "                               (dESGF.variable_id==variable_id)]\n",
    "                \n",
    "                if members == 'All':\n",
    "                    member_ids = dESGF.member_id.unique()\n",
    "                else:\n",
    "                    member_ids = members\n",
    "\n",
    "                for member_id in member_ids:\n",
    "                    dfm = df_grid[df_grid.member_id==member_id]\n",
    "                    grid_labels = dfm.grid_label.unique()\n",
    "                    for grid_label in grid_labels:\n",
    "                        dfmg = dfm[dfm.grid_label==grid_label]\n",
    "                    \n",
    "                        file = dfmg.values[0]\n",
    "                        zarr_dir = dict(zip(dfmg.keys(),file))\n",
    "                        zarr_file = zarr_format % zarr_dir \n",
    "\n",
    "                        zstore = 'gs://cmip6' + zarr_file \n",
    "\n",
    "                        #print(zstore)\n",
    "                        df_cloud = dzLocal[(dzLocal.zstore==zstore)]\n",
    "\n",
    "                        if len(df_cloud) >= 1:\n",
    "                            print('store already in cloud')\n",
    "                            continue\n",
    "                        else:\n",
    "                            ngood += 1\n",
    "\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.filterwarnings(\"ignore\")\n",
    "                            dfmg.loc[:,'zstore'] = zarr_file\n",
    "\n",
    "                        df_list += [dfmg]\n",
    "\n",
    "    dESGF3 = pd.concat(df_list,sort=False)\n",
    "    dESGF3.to_csv('csv/ESGF3.csv',index=False)\n",
    "else:\n",
    "    dESGF3 = pd.read_csv('csv/ESGF3.csv')\n",
    "\n",
    "keys_all = list(dESGF3.keys())\n",
    "keys_show = [\"source_id\",\"experiment_id\",\"member_id\",\"variable_id\",'zstore']\n",
    "keys_drop = list(set(keys_all) - set(keys_show))\n",
    "\n",
    "dESGF3.drop(keys_drop,1).groupby(['experiment_id','variable_id','source_id','member_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed = dESGF3\n",
    "\n",
    "if len(df_needed) > 0:\n",
    "    num_stores = df_needed.zstore.nunique() \n",
    "    print(f'needed: nfiles={len(df_needed)}, nstores={num_stores}')\n",
    "    #print(df_needed.zstore.unique())\n",
    "else:\n",
    "    print('no new data available')\n",
    "    exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_needed.zstore.unique()\n",
    "print(\"table_id = '\",*df_needed.table_id.unique(),\"'\",sep = \"\")\n",
    "print('exps = [\\'',end=\"\"); print(*df_needed.experiment_id.unique(), sep = \"','\",end=\"\" ); print('\\']')\n",
    "print('variables = [\\'',end=\"\"); print(*df_needed.variable_id.unique(), sep = \"','\",end=\"\" ); print('\\']')\n",
    "print('members = [\\'',end=\"\"); print(*df_needed.member_id.unique(), sep = \"','\",end=\"\" ); print('\\']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start logging the progress and exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(datetime.datetime.now().strftime(\"%Y%m%d%s\"))\n",
    "\n",
    "cat_file = 'csv/cmip6_'+date+'.csv'\n",
    "log_file = 'txt/request_'+date+'.log'\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and close for each write in case of kernel interrupt\n",
    "def write_log(file,str,verbose=True):\n",
    "    f = open(file,'a')\n",
    "    if verbose:\n",
    "        print(str)\n",
    "    f.write(str+'\\n')\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed['member'] = [int(s.split('r')[-1].split('i')[0]) for s in df_needed['member_id']]\n",
    "df_needed = df_needed.sort_values(by=['source_id'])\n",
    "df_needed = df_needed.sort_values(by=['member'])\n",
    "#df_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List sites to skip for aquiring new netcdf files: broken or slow sites\n",
    "\n",
    "skip_sites = []\n",
    "skip_sites += ['esg.lasg.ac.cn']\n",
    "#skip_sites += ['esgf.nci.org.au']\n",
    "#skip_sites += ['esg-cccr.tropmet.res.in']\n",
    "#skip_sites += ['esgf.ichec.ie']\n",
    "#skip_sites += ['esgf-data3.ceda.ac.uk']\n",
    "#skip_sites += []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### The real work is done in this next loop \n",
    "- could be done in parallel except for the writing to the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the catalog\n",
    "df_GCS = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype='unicode')\n",
    "\n",
    "# refresh the gcsfs\n",
    "fs.invalidate_cache()\n",
    "\n",
    "new_zarrs = df_needed.zstore.unique()\n",
    "\n",
    "verbose = True\n",
    "\n",
    "zdict = {}  # construct dictionary for new rows to add to master catalog\n",
    "for item,zarr in enumerate(new_zarrs):\n",
    "    \n",
    "    zbdir  = zarr_local  + zarr\n",
    "    \n",
    "    write_log(log_file,f\"\\n>>{item+1}/{num_stores}:<< local file: {zbdir}\",verbose=verbose)\n",
    "    \n",
    "    # is zarr already in cloud?\n",
    "    gsurl = 'gs://cmip6' + zarr\n",
    "    contents = fs.ls(gsurl)\n",
    "    if any(\"zmetadata\" in s for s in contents):\n",
    "        write_log(log_file,'store already in cloud',verbose=verbose)\n",
    "        continue\n",
    "\n",
    "    cstore = df_GCS[df_GCS.zstore == gsurl]\n",
    "\n",
    "    if len(cstore) > 0:\n",
    "        print('store already in cloud catalog')\n",
    "        continue\n",
    "\n",
    "    # has zarr store already been created locally?  \n",
    "    zstrs = glob(zbdir + '/.zmetadata')\n",
    "    if len(zstrs) > 0 :\n",
    "        print('store already exists locally, but is not in cloud, please upload manually')\n",
    "        continue     \n",
    "            \n",
    "    # Download the needed netcdf files - reading the known trouble codes from database\n",
    "    gfiles,troubles,codes,okay = get_ncfiles(zarr,df_needed,skip_sites)\n",
    "    \n",
    "    write_log(log_file,troubles,verbose=verbose)\n",
    "    \n",
    "    if okay == False:\n",
    "        continue\n",
    "\n",
    "    if len(gfiles) == 0: \n",
    "        write_log(log_file,'no files available',verbose=verbose)\n",
    "        continue\n",
    "    \n",
    "    variable_id = zarr.split('/')[-3]\n",
    "\n",
    "    # concatenate in time with mfdataset\n",
    "    gfiles = sorted(gfiles)\n",
    "    status, ds, dstr = concatenate(zarr,gfiles,codes)  \n",
    "\n",
    "    if status == 'failure':\n",
    "        write_log(log_file,status+dstr,verbose=verbose)\n",
    "        continue\n",
    "    else:\n",
    "        write_log(log_file,dstr)\n",
    "\n",
    "    # convert to zarr, with consolidated metadata\n",
    "    ds.to_zarr(zbdir, consolidated=True, mode='w')\n",
    "        \n",
    "    if not os.path.isfile(zbdir+'/.zmetadata'):\n",
    "        write_log(log_file,'to_zarr failure: ',verbose=verbose)\n",
    "        continue\n",
    "   \n",
    "    vlist = get_details(ds,zbdir,zarr)\n",
    "    \n",
    "    # upload to cloud\n",
    "    command = '/usr/bin/gsutil -m cp -r '+ zbdir[:-1] + ' ' + gsurl[:-1]\n",
    "    write_log(log_file,command,verbose=verbose)\n",
    "    # uncomment next line to really upload to GC\n",
    "    os.system(command) \n",
    "        \n",
    "    size_remote = fs.du(gsurl)\n",
    "    size_local = getFolderSize(zbdir)\n",
    "    assert (size_remote - size_local) < 100\n",
    "    write_log(log_file,f'uploaded {zbdir} correctly',verbose=verbose)    \n",
    "\n",
    "    try:\n",
    "        ds = xr.open_zarr(fs.get_mapper(gsurl), consolidated=True)\n",
    "        zdict[item] = vlist\n",
    "        write_log(log_file,f'successfully saved as {zbdir}')\n",
    "        for gfile in gfiles:\n",
    "            os.system('rm -f '+ gfile)\n",
    "    except:\n",
    "        write_log(log_file,'store did not get saved to GCS properly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(zdict) == 0 :\n",
    "    print('nothing else to do')\n",
    "    exit\n",
    "else:\n",
    "    dz = dict_to_dfcat(zdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now update cloud catalog to reflect this new data (nb3b-CloudCat.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo-Oct2019",
   "language": "python",
   "name": "pangeo-oct2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
