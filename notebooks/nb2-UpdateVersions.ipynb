{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update datasets in GC which have newer versions available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "from IPython.display import display\n",
    "from glob import glob\n",
    "import warnings\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from request import requests, set_request_id\n",
    "from search import search_new, esgf_search_sites\n",
    "from netcdf import get_ncfiles, concatenate\n",
    "from identify import needed_newversion, get_version\n",
    "from response import response, dict_to_dfcat, get_details\n",
    "from utilities import getFolderSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to write NEW zarr stores:\n",
    "\n",
    "mach = os.uname()[1]\n",
    "if 'haden' in mach:\n",
    "    local_storage = True\n",
    "    zarr_local = '/h68/naomi/zarr-minimal'\n",
    "else:\n",
    "    local_storage = False\n",
    "    zarr_local = '/d1/naomi/cmip6-zarrs'  # usually matches location in nb1-DataRequests\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose basic configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = esgf_search_sites()\n",
    "\n",
    "print('possible ESGF API search nodes: ',list(dtype.keys()))\n",
    "\n",
    "local_node=False\n",
    "ESGF_site = dtype['llnl'];local_node = True\n",
    "#ESGF_site = dtype['dkrz'];local_node = False\n",
    "#ESGF_site = dtype['ipsl'];local_node = False\n",
    "#ESGF_site = dtype['ceda'];local_node = False\n",
    "\n",
    "# List sites to skip for aquiring new netcdf files: broken or slow sites\n",
    "skip_sites = []#'esg.lasg.ac.cn','esgf-data2.diasjp.net','esgf-cnr.hpc.cineca.it'] #['dist.nmlab.snu.ac.kr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prior Google Sheet requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior = pd.read_csv('csv/requests.csv')\n",
    "#df_prior\n",
    "#df_prior.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new Google Sheet requests\n",
    "- by default, only the new rows from the sheet are considered\n",
    "- specifying a list of rows or emails will add older entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []   \n",
    "emails = []\n",
    "\n",
    "# modify here:\n",
    "\n",
    "rows = [199]\n",
    "\n",
    "#emails = ['neil.swart@canada.ca']\n",
    "df_request_new, dtrouble = requests(df_prior,rows=rows,emails=emails)\n",
    "\n",
    "request_id = set_request_id()\n",
    "\n",
    "# Check for mal-formed requests (non-existent variables, etc)\n",
    "if len(dtrouble)>=1:\n",
    "    print(dtrouble)\n",
    "\n",
    "df_request_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a new request to process:\n",
    "timestamps = df_request_new.Timestamp.unique()\n",
    "print(timestamps)\n",
    "df_request_new = df_request_new[df_request_new.Timestamp == timestamps[-1]]\n",
    "df_request_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search ESGF for the availability of requested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewSearch=True\n",
    "\n",
    "if NewSearch:\n",
    "    print(ESGF_site)\n",
    "    df_ESGF = search_new(ESGF_site,df_request_new,local_node=local_node)\n",
    "    df_ESGF.to_csv('csv/ESGF_UV.csv',index=False)\n",
    "else:\n",
    "    df_ESGF = pd.read_csv('csv/ESGF_UV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the master list of existing zarr stores\n",
    "- df_master includes all curated stores\n",
    "- df_avail includes all stores, even those with known ES-DOC issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avail = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv')\n",
    "len(df_avail),len(df_ESGF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the new requests:\n",
    "- exists in df_ESGF (what is available)? if yes, continue\n",
    "- already exists in df_avail (what we have)? if yes, check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ESGF_site)\n",
    "\n",
    "df_needed = needed_newversion(df_avail, df_request_new, df_ESGF)\n",
    "\n",
    "if len(df_needed) > 0:\n",
    "    num_stores = df_needed.zstore.nunique() \n",
    "    print(f'needed: nfiles={len(df_needed)}, nstores={num_stores}')\n",
    "    #print(df_needed.zstore.unique())\n",
    "else:\n",
    "    print('no new data available')\n",
    "    exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE - the ESGF API and web search pages give different results!!!\n",
    "skipping due to multiple esgf versions: /CMIP/CCCma/CanESM5/historical/r1i1p1f1/Amon/pr/gn/ ['v20190306' 'v20190429']\n",
    "    The API returns both versions (version=latest DOES NOT WORK if a version has been retracted!!!), \n",
    "but ESGF search page only has one if other has been retracted retracted\n",
    "    1.\t\n",
    "    CMIP6.CMIP.CCCma.CanESM5.historical.r1i1p1f1.Amon.pr.gn\n",
    "    IMPORTANT: this dataset has been retracted and is no longer available for download\n",
    "    Data Node: crd-esgf-drc.ec.gc.ca\n",
    "    Version: 20190306\n",
    "    ÃŸ Full Dataset Services:   [ Show Metadata ]\n",
    "    2.\t\n",
    "    CMIP6.CMIP.CCCma.CanESM5.historical.r1i1p1f1.Amon.pr.gn\n",
    "    Data Node: crd-esgf-drc.ec.gc.ca\n",
    "    Version: 20190429\n",
    "    Total Number of Files (for all variables): 1\n",
    "    Full Dataset Services:  \t[ Show Metadata ]   [ List Files ]   [ WGET Script ]   [ LAS ]   [ Show Citation ]   [ PID ] [ Further Info ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start logging the progress and exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_file = 'csv/cmip6_'+request_id+'.csv'\n",
    "log_file = 'txt/request_'+request_id+'.log'\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and close for each write in case of kernel interrupt\n",
    "def write_log(file,str,verbose=True):\n",
    "    f = open(file,'a')\n",
    "    if verbose:\n",
    "        print(str)\n",
    "    f.write(str+'\\n')\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed['member'] = [int(s.split('r')[-1].split('i')[0]) for s in df_needed['member_id']]\n",
    "df_needed = df_needed.sort_values(by=['source_id'])\n",
    "df_needed = df_needed.sort_values(by=['member'])\n",
    "df_needed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### The real work is done in this next loop "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if new version is available, download and create new zarr store - if successful then,\n",
    "    0. save new zstore in zbtemp\n",
    "    1. delete old version in GC\n",
    "    2. delete entry in ncsv/GC_files_{activity_id}-{institution_id}.csv\n",
    "    if saving local copy:\n",
    "        3. delete old local copy(ies)\n",
    "        4. delete entry(ies) in shelf-new/h*.csv\n",
    "        5. copy zarr from zbtemp to zbdir\n",
    "    6. upload to cloud\n",
    "    7. fix version name in LOCAL COPY of noQC catalog (df_GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List sites to skip for aquiring new netcdf files: broken or slow sites\n",
    "\n",
    "skip_sites = []\n",
    "skip_sites += ['esg.lasg.ac.cn']\n",
    "#skip_sites += ['esgf.nci.org.au']\n",
    "skip_sites += ['esg-cccr.tropmet.res.in']\n",
    "skip_sites += ['esgf.ichec.ie']  # incomplete downloads\n",
    "#skip_sites += ['esgf-data3.ceda.ac.uk']\n",
    "skip_sites += ['dpesgf03.nccs.nasa.gov']\n",
    "#skip_sites += ['esgf3.dkrz.de']\n",
    "skip_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some new versions are IDENTICAL to old - same checksums, tracking_ids, etc. \n",
    "# WHAT TO DO???? Will keep trying to download again and again\n",
    "skip_stores = ['gs://cmip6/CMIP/E3SM-Project/E3SM-1-1-ECA/historical/r1i1p1f1/Amon/prc/gr/'] # new version same as old\n",
    "skip_stores += ['gs://cmip6/CMIP/E3SM-Project/E3SM-1-1-ECA/historical/r1i1p1f1/Amon/pr/gr/'] # new version same as old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the catalog\n",
    "df_GCS = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype='unicode')\n",
    "\n",
    "date = str(datetime.datetime.now().strftime(\"%Y%m%d\"))\n",
    "os.system('cp /home/naomi/cmip6-zarr/csv/pangeo-cmip6-noQC.csv /home/naomi/cmip6-zarr/csv/pangeo-cmip6-'+date+'-noQC.csv')\n",
    "\n",
    "if local_storage:\n",
    "    df_loc = pd.read_csv('shelf-new/local.csv', dtype='unicode')\n",
    "    zbdirs = []\n",
    "    for i in range(1,84):\n",
    "        zbdirs += ['/h'+str(i)]\n",
    "\n",
    "# refresh the gcsfs\n",
    "fs.invalidate_cache()\n",
    "\n",
    "new_zarrs = df_needed.zstore.unique()\n",
    "\n",
    "verbose = True\n",
    "for item,zarr in enumerate(new_zarrs):\n",
    "   \n",
    "    zbdir  = zarr_local  + zarr\n",
    "    zbtemp = 'zbtemp' + zarr\n",
    "    gsurl = 'gs://cmip6' + zarr\n",
    "    \n",
    "    if gsurl in skip_stores:\n",
    "        print('Troublesome data - new version same as old',gsurl)\n",
    "        continue\n",
    "        \n",
    "    write_log(log_file,f\"\\n>>{item+1}/{num_stores}:<< local file: {zbdir}\",verbose=verbose)\n",
    "    \n",
    "    try:\n",
    "        dataset_cloud, version_cloud = get_version(fs.get_mapper(gsurl),method='none')   \n",
    "        # caching trouble using fsspec!!, use the fs directly\n",
    "    except:\n",
    "        version_cloud = 'unknown'\n",
    "    \n",
    "    cstore = df_GCS[df_GCS.zstore == gsurl]\n",
    "    if len(cstore) > 0:\n",
    "        print(cstore.zstore.values,'version in GCS catalog:',cstore.version.values)\n",
    "        print('store in cloud catalog') \n",
    "        \n",
    "    new_version = df_needed[df_needed.zstore==zarr].version.unique()[0][1:]\n",
    "    \n",
    "    print('available version:',new_version,'cloud version, using gcsfs:',version_cloud)\n",
    "    if int(new_version) <= int(version_cloud):\n",
    "        print('new(er) version already uploaded to cloud',version_cloud)\n",
    "        continue\n",
    "        \n",
    "    # CHECK IN shelf-new/local.csv\n",
    "    if local_storage:\n",
    "        df1 = df_loc[df_loc['zstore'].str.contains(zarr[:-1])]\n",
    "\n",
    "    gfiles,troubles,codes,okay = get_ncfiles(zarr,df_needed,skip_sites)\n",
    "    \n",
    "    write_log(log_file,troubles,verbose=verbose)\n",
    "    \n",
    "    if okay == False:\n",
    "        continue\n",
    "\n",
    "    if len(gfiles) == 0: \n",
    "        write_log(log_file,'no files available',verbose=verbose)\n",
    "        continue\n",
    "    \n",
    "    variable_id = zarr.split('/')[-3]\n",
    "    for gfile in gfiles:   # changes file sizes!!\n",
    "        command = '/usr/bin/ncatted -h -O -a missing_value\\,'+variable_id+',d,, '+gfile\n",
    "        os.system(command)\n",
    "\n",
    "    # concatenate in time with mfdataset\n",
    "    #print(gfiles)\n",
    "    gfiles = sorted(gfiles)\n",
    "    status, ds, dstr = concatenate(zarr,gfiles,codes)  \n",
    "\n",
    "    if status == 'failure':\n",
    "        write_log(log_file,status+dstr,verbose=verbose)\n",
    "        continue\n",
    "    else:\n",
    "        write_log(log_file,dstr)\n",
    "\n",
    "    # 0. save new zstore\n",
    "    try:\n",
    "        del ds[variable_id].encoding['missing_value']\n",
    "        ds.to_zarr(zbtemp, consolidated=True, mode='w')\n",
    "    except:\n",
    "        ds.to_zarr(zbtemp, consolidated=True, mode='w')\n",
    "        \n",
    "    if not os.path.isfile(zbtemp+'/.zmetadata'):\n",
    "        write_log(log_file,'to_zarr failure: ',verbose=verbose)\n",
    "        continue\n",
    "     \n",
    "    # 1. delete old version in GC\n",
    "    command = '/usr/bin/gsutil -m rm -r '+ gsurl[:-1]\n",
    "    print(command)\n",
    "    os.system(command) \n",
    "    \n",
    "    # 2. delete entry in ncsv/GC_files_{activity_id}-{institution_id}.csv\n",
    "    activity_id = cstore.activity_id.values[0]\n",
    "    institution_id = cstore.institution_id.values[0]\n",
    "    \n",
    "    file = f'/home/naomi/cmip6-zarr/ncsv/GC_files_{activity_id}-{institution_id}.csv'\n",
    "    #print('modifying ',file)\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    with open(file, \"w\") as f:\n",
    "        for line in lines:\n",
    "            if line.strip(\"\\n\") != gsurl + \".zmetadata\":\n",
    "                f.write(line)\n",
    "        \n",
    "    if local_storage:\n",
    "        # 3. delete old local copy(ies)\n",
    "        for zloc in df1.zstore.values:\n",
    "            if os.path.exists(zloc):\n",
    "                command = '/bin/rm -rf '+ zloc\n",
    "                print(command)\n",
    "                os.system(command)\n",
    "            else:\n",
    "                print('not active: ',zloc)\n",
    "    \n",
    "        # 4. delete entry(ies) in shelf-new/h*.csv\n",
    "        for zloc in df1.zstore.values:\n",
    "            file = 'shelf-new/' + zloc.split('/')[1] + '.csv'\n",
    "            print(file)\n",
    "            writeable = os.access(file, os.W_OK)\n",
    "            if not writeable:\n",
    "                command = \"chmod u+w \" + file\n",
    "                #print(command)\n",
    "                os.system(command)\n",
    "\n",
    "            dfff = pd.read_csv(file, dtype='unicode')\n",
    "            dff = dfff[dfff.zstore != zloc]\n",
    "            dff.to_csv(file, mode='w+', index=False)\n",
    "\n",
    "            if not writeable:\n",
    "                command = \"chmod u-w \" + file\n",
    "                #print(command)\n",
    "                os.system(command)\n",
    "               \n",
    "            # remove from concatenated catalog\n",
    "            file = 'shelf-new/local.csv'\n",
    "            dfff = pd.read_csv(file, dtype='unicode')\n",
    "            dff = dfff[dfff.zstore != zloc]\n",
    "            dff.to_csv(file, mode='w+', index=False)\n",
    "\n",
    "\n",
    "        # 5. copy zbtemp to zbdir\n",
    "        grid_label = zbdir[:-1].split('/')[-1]\n",
    "        command = 'mkdir -p ' + zbdir.split(grid_label)[0]\n",
    "        os.system(command) \n",
    "        command = '/bin/cp -r '+ zbtemp[:-1] + ' ' + zbdir[:-1]\n",
    "        write_log(log_file,command,verbose=verbose)\n",
    "        os.system(command) \n",
    "    \n",
    "    # 6. upload to cloud\n",
    "    command = '/usr/bin/gsutil -m cp -r '+ zbtemp[:-1] + ' ' + gsurl[:-1]\n",
    "    write_log(log_file,command,verbose=verbose)\n",
    "    os.system(command) \n",
    "        \n",
    "    size_remote = fs.du(gsurl)\n",
    "    size_local = getFolderSize(zbdir)\n",
    "    assert (size_remote - size_local) < 100\n",
    "\n",
    "    try:\n",
    "        ds = xr.open_zarr(fs.get_mapper(gsurl), consolidated=True)\n",
    "        write_log(log_file,f'successfully saved as {gsurl}')\n",
    "        for gfile in gfiles:\n",
    "            os.system('rm -f '+ gfile)\n",
    "        #os.system('rm -f '+ zbtemp)\n",
    "    except:\n",
    "        write_log(log_file,'store did not get saved to GCS properly')\n",
    "    \n",
    "\n",
    "    # 7. fix version in local noQC catalog  (only update cloud catalog when finished)\n",
    "    \n",
    "    df = pd.read_csv('csv/pangeo-cmip6-noQC.csv', dtype='unicode')\n",
    "    idx = df.index[df['zstore'] == gsurl]\n",
    "    df.at[idx, 'version'] = new_version\n",
    "    df.to_csv('csv/pangeo-cmip6-noQC.csv', mode='w+', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When done, update the GCS noQC catalog with the modified local one\n",
    "ret = os.system('/usr/bin/gsutil -m cp csv/pangeo-cmip6-noQC.csv gs://cmip6/cmip6-zarr-consolidated-stores-noQC.csv')\n",
    "if ret != 0:\n",
    "    print('noQC upload not working')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo-Oct2019",
   "language": "python",
   "name": "pangeo-oct2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
