{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle New Data Requests Automatically\n",
    "- beginning of notebook is assumed to be interactive until the requests have been checked\n",
    "- all progress and exception logging is done only for main loop\n",
    "- copy and paste the e-mail response and send from gcs.cmip6.ldeo@gmail.com account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "from IPython.display import display\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from request import requests, set_request_id\n",
    "from search import search, esgf_search_sites\n",
    "from netcdf import get_ncfiles, concatenate\n",
    "from identify import needed\n",
    "from response import response, dict_to_dfcat, get_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFolderSize(p):\n",
    "    prepend = partial(os.path.join, p)\n",
    "    return sum([(os.path.getsize(f) if os.path.isfile(f) else getFolderSize(f)) for f in map(prepend, os.listdir(p))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to write local zarr stores:\n",
    "zarr_local = '/d1/naomi/zarrs'\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose basic configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = esgf_search_sites()\n",
    "\n",
    "local_node = False\n",
    "\n",
    "print('possible ESGF API search nodes: ',list(dtype.keys()))\n",
    "\n",
    "ESGF_site = dtype['llnl'];local_node = True\n",
    "#ESGF_site = dtype['dkrz']\n",
    "#ESGF_site = dtype['ipsl']\n",
    "#ESGF_site = dtype['ceda'];local_node = False  # CEDA doesn't allow local-only searches\n",
    "\n",
    "# List data nodes to skip for aquiring new netcdf files: broken or slow sites\n",
    "skip_sites = ['esg.lasg.ac.cn','esgf-data2.diasjp.net','esgf-cnr.hpc.cineca.it'] #['dist.nmlab.snu.ac.kr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete archive of CMIP6 output is made available for search and download via any one of the following portals:\n",
    "\n",
    "USA, PCMDI/LLNL (California) - https://esgf-node.llnl.gov/search/cmip6/\n",
    "\n",
    "France, IPSL - https://esgf-node.ipsl.upmc.fr/search/cmip6-ipsl/\n",
    "\n",
    "Germany, DKRZ - https://esgf-data.dkrz.de/search/cmip6-dkrz/\n",
    "\n",
    "UK, CEDA - https://esgf-index1.ceda.ac.uk/search/cmip6-ceda/\n",
    "\n",
    "If you encounter slow responses from one search interface, you might try one of the other portals (perhaps one near you). Also note that the datasets themselves are stored (and partially replicated) on a federated system of data nodes, and again you may find differences from node to node in download speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prior Google Sheet requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior = pd.read_csv('csv/requests.csv')\n",
    "df_prior.keys()\n",
    "df_prior.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new Google Sheet requests\n",
    "https://docs.google.com/spreadsheets/d/1SGTSK_h4xWX3gdgpeWeCpL_vhzf6tnGPmxetO1gOlQc/edit?usp=sharing\n",
    "- by default, only the new rows from the sheet are considered\n",
    "- specifying a list of rows or emails will add older entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []   \n",
    "emails = []\n",
    "\n",
    "# modify here:\n",
    "rows = [184]  # GithubTest\n",
    "#emails = ['neil.swart@canada.ca']\n",
    "\n",
    "df_request_new, dtrouble = requests(df_prior,rows=rows,emails=emails)\n",
    "request_id = set_request_id()\n",
    "\n",
    "# Print mal-formed requests (non-existent variables, etc)\n",
    "if len(dtrouble)>=1:\n",
    "    print(dtrouble)\n",
    "\n",
    "df_request_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a new request to process:\n",
    "timestamps = df_request_new.Timestamp.unique()\n",
    "print(timestamps)\n",
    "df_request_new = df_request_new[df_request_new.science == 'GithubTest']\n",
    "\n",
    "df_request_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_request_new.comments.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search ESGF for the availability of requested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ESGF_site)\n",
    "df_ESGF = search(ESGF_site,df_request_new,local_node=local_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the master list of existing zarr stores\n",
    "- df_avail includes all stores, EVEN THOSE with known ES-DOC issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avail = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype={'version': 'unicode'})\n",
    "len(df_avail),len(df_ESGF)\n",
    "\n",
    "df_ESGF.HTTPServer_url.values[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the new requests:\n",
    "- already exists in df_avail (what we have) - not needed\n",
    "- exists in df_ESGF (what is available) - if not available, then not needed|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed = needed(df_avail, df_request_new, df_ESGF)\n",
    "\n",
    "if len(df_needed) > 0:\n",
    "    num_stores = df_needed.zstore.nunique() \n",
    "    print(f'needed: nfiles={len(df_needed)}, nstores={num_stores}')\n",
    "    #print(df_needed.zstore.unique())\n",
    "else:\n",
    "    print('no new data available')\n",
    "    exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_needed.zstore.unique()\n",
    "print(\"table_id = '\",*df_needed.table_id.unique(),\"'\",sep = \"\")\n",
    "print('exps = [\\'',end=\"\"), print(*df_needed.experiment_id.unique(), sep = \"','\",end=\"\" ), print('\\']')\n",
    "print('variables = [\\'',end=\"\"), print(*df_needed.variable_id.unique(), sep = \"','\",end=\"\" ), print('\\']')\n",
    "#print('members = [\\'',end=\"\"), print(*df_needed.member_id.unique(), sep = \"','\",end=\"\" ), print('\\']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start logging the progress and exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_file = 'csv/cmip6_'+request_id+'.csv'\n",
    "log_file = 'txt/request_'+request_id+'.log'\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and close for each write in case of kernel interrupt\n",
    "def write_log(file,str,verbose=True):\n",
    "    f = open(file,'a')\n",
    "    if verbose:\n",
    "        print(str)\n",
    "    f.write(str+'\\n')\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed['member'] = [int(s.split('r')[-1].split('i')[0]) for s in df_needed['member_id']]\n",
    "df_needed = df_needed.sort_values(by=['source_id'])\n",
    "df_needed = df_needed.sort_values(by=['member'])\n",
    "#df_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### The real work is done in this next loop \n",
    "- could be done in parallel except for the writing to the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the catalog\n",
    "df_GCS = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype='unicode')\n",
    "\n",
    "# refresh the gcsfs\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only',cache_timeout=-1)\n",
    "\n",
    "new_zarrs = df_needed.zstore.unique()\n",
    "\n",
    "verbose = True\n",
    "\n",
    "zbdirs = []\n",
    "for i in range(1,84):\n",
    "    zbdirs += ['/h'+str(i)]\n",
    "\n",
    "zdict = {}  # construct dictionary for new rows to add to master catalog\n",
    "for item,zarr in enumerate(new_zarrs):\n",
    "    \n",
    "    zbdir  = zarr_local  + zarr\n",
    "    \n",
    "    write_log(log_file,f\"\\n>>{item+1}/{num_stores}:<< local file: {zbdir}\",verbose=verbose)\n",
    "    \n",
    "    # is zarr already in cloud?\n",
    "    gsurl = 'gs://cmip6' + zarr\n",
    "    contents = fs.ls(gsurl)\n",
    "    if any(\"zmetadata\" in s for s in contents):\n",
    "        write_log(log_file,'store already in cloud',verbose=verbose)\n",
    "        continue\n",
    "\n",
    "    cstore = df_GCS[df_GCS.zstore == gsurl]\n",
    "\n",
    "    if len(cstore) > 0:\n",
    "        print('store already in cloud catalog')  # or on a shelf drive which is already in cloud\n",
    "        continue\n",
    "\n",
    "    # does zarr exist on active drives?  \n",
    "    zstrs = glob('/h*/naomi/zarr-minimal' + zarr + '/.zmetadata')\n",
    "    if len(zstrs) > 0 :\n",
    "        print('store already exists locally, but not in cloud')\n",
    "        continue       \n",
    "            \n",
    "    # Download the needed netcdf files - reading the known trouble codes from database\n",
    "    gfiles,troubles,codes,okay = get_ncfiles(zarr,df_needed,skip_sites)\n",
    "    \n",
    "    write_log(log_file,troubles,verbose=verbose)\n",
    "    \n",
    "    if okay == False:\n",
    "        continue\n",
    "\n",
    "    if len(gfiles) == 0: \n",
    "        write_log(log_file,'no files available',verbose=verbose)\n",
    "        continue\n",
    "    \n",
    "    variable_id = zarr.split('/')[-3]\n",
    "\n",
    "    # concatenate in time with mfdataset\n",
    "    gfiles = sorted(gfiles)\n",
    "    status, ds, dstr = concatenate(zarr,gfiles,codes)  \n",
    "\n",
    "    if status == 'failure':\n",
    "        write_log(log_file,status+dstr,verbose=verbose)\n",
    "        continue\n",
    "    else:\n",
    "        write_log(log_file,dstr)\n",
    "\n",
    "    # convert to zarr, with consolidated metadata\n",
    "    ds.to_zarr(zbdir, consolidated=True, mode='w')\n",
    "        \n",
    "    if not os.path.isfile(zbdir+'/.zmetadata'):\n",
    "        write_log(log_file,'to_zarr failure: ',verbose=verbose)\n",
    "        continue\n",
    "   \n",
    "    vlist = get_details(ds,zbdir,zarr)\n",
    "    \n",
    "    # upload to cloud\n",
    "    command = '/usr/bin/gsutil -m cp -r '+ zbdir[:-1] + ' ' + gsurl[:-1]\n",
    "    write_log(log_file,command,verbose=verbose)\n",
    "    # uncomment next line to really upload to GC\n",
    "    # os.system(command) \n",
    "        \n",
    "    size_remote = fs.du(gsurl)\n",
    "    size_local = getFolderSize(zbdir)\n",
    "    assert (size_remote - size_local) < 100\n",
    "    write_log(log_file,f'uploaded {zbdir} correctly',verbose=verbose)    \n",
    "\n",
    "    try:\n",
    "        ds = xr.open_zarr(fs.get_mapper(gsurl), consolidated=True)\n",
    "        zdict[item] = vlist\n",
    "        write_log(log_file,f'successfully saved as {zbdir}')\n",
    "        for gfile in gfiles:\n",
    "            os.system('rm -f '+ gfile)\n",
    "    except:\n",
    "        write_log(log_file,'store did not get saved to GCS properly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(zdict) == 0 :\n",
    "    print('nothing else to do')\n",
    "    exit\n",
    "else:\n",
    "    dz = dict_to_dfcat(zdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a table of acquired data to send in email to requestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_master_new = pd.concat([df_avail, dz],sort=True)\n",
    "except:\n",
    "    df_master_new = df_avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldict = []\n",
    "names = \"\"\n",
    "print('Re: CMIP6 GCS Data Request (Responses)')\n",
    "for row in df_request_new.values:\n",
    "    rdict = dict(zip(df_request_new.keys(),row))\n",
    "    #print(rdict)\n",
    "    name = rdict['requester']\n",
    "    timestamp = rdict['Timestamp']\n",
    "    names += name\n",
    "    del rdict['response status']\n",
    "    ldict += [rdict]\n",
    "    dfr = df_request_new[df_request_new.Timestamp == timestamp]\n",
    "    \n",
    "    print('Dear',name+':')\n",
    "    print('\\n  Here are the results from your recent CMIP6 data request(s).  The master catalog, https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores.csv, will be updated with the nightly build.')\n",
    "    #if len(dtrouble)>=1:\n",
    "    #    print('\\n '+dtrouble)\n",
    "    print('\\n  Please note: ')\n",
    "    print('      - Data for some models (e.g., CAS/FGOALS-f3-L and NUIST/NESM3) must be obtained directly from servers which are too slow or unresponsive. ')\n",
    "    print('      - We exclude data with known errors (as reported at ES-DOC) from the official listing at https://errata.es-doc.org/ .')\n",
    "    print('        However, data labelled status=resolved or severity=low are included in the master catalog.')\n",
    "    \n",
    "\n",
    "    print('      - Some data we have not been able to clean up enough to get it concatenated and save to zarr. Other datasets are only available for disjointed time periods.')\n",
    "    print('\\n  See the sample Jupyter Notebook at https://gist.github.com/naomi-henderson/ed1801d8ee8b992dda252f8b126876a5 for a quick introduction to accessing the data.')\n",
    "    print('\\nFrom the folks at:\\n  The Climate Data Science Lab\\n  Division of Ocean and Climate Physics\\n  LDEO/Columbia University')\n",
    "    print('\\n--------------------------')\n",
    "\n",
    "    print('\\nrequest:')\n",
    "    display(rdict)\n",
    "\n",
    "    print('\\nresponse:')\n",
    "    try:\n",
    "        print('new stores added:\\n',len(dz),'\\n')\n",
    "    except:\n",
    "        print(f'no new data available at ESGF API search node {ESGF_site}')\n",
    "\n",
    "    #print('\\n',dfr,len(df_master_new))\n",
    "    table = response(dfr,df_master_new)\n",
    "\n",
    "    print(\"\\navailable data:\\n  this includes your new stores but does not include datasets marked 'onhold', 'wontfix' or 'new' in the ES-DOC ERRATA\")\n",
    "    display(table)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace request database with new database\n",
    "! mv csv/request_new.csv csv/requests.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPython3.6",
   "language": "python",
   "name": "my3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
